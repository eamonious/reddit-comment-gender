{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do We Represent Language So Computers Can Analyze It?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to analyze Reddit comments to make some prediction about them, we need to first find a way to **vectorize**, or numerically represent, the text of the comments.  This is the first concern of **natural language processing (NLP)**.  The entire comment collection taken together is referred to as the **corpus**, from the Latin for body.\n",
    "\n",
    "One common approach to vectorizing words is called the **bag of words** representation.  In this strategy, we don't care about the relative positions of any of the words in a comment.  All we do is break the words into units (**tokenization**), count how often a given word appears in each comment, and then create a vector for each comment that is just the number of times each word appears, including all the 0s for words that appear somewhere in the corpus, but don't appear in that particular comment.  \n",
    "When we do this, we usually want to exclude words that are extremely common, like \"the\", \"and\", etc... these words dominate frequencies, but their influence on the meaning of the text is usually minimal.  So it's better to leave them out if we're trying to make accurate predictions.  In NLP, these words are called **stop words**, and there are standard exclusion lists that are built into the available vectorizers as options.\n",
    "\n",
    "There are a few different vectorizers to choose from:  **CountVectorizer()** and **TfidfVectorizer()** are the most widely used.  CountVectorizer tokenizes and counts, and that's it.  TfIdfVectorizer goes a step further, and **normalizes** the frequencies.  Basically, TfidfVectorizer will apply a transformation to our comment vectors that will **down-weight the influence of words that appear in a lot of comments, while up-weighting the influence of rarer words**.  The idea behind this is that common words tend to be less interesting to us; they give us less information about the comment.  Rarer words are more likely to have predictive value. \n",
    "\n",
    "That said, if a word is *too rare*, and only appears in one or two comments, there isn't much point to including it - what patterns can it really give us?  There are parameters that will allow us to only include words in our analysis if they appear in at least *n* comments.  \n",
    "\n",
    "Single words may be able to help us predict whether a comment is AskMen or AskWomen, but what if we want to consider the frequency of certain *pairs* of words, or *triads*?  This is where **n-grams**, or groups of n words, come into play.  We can run our model with different combinations of n-grams included and see which performs best.  In this case, we found the best performance with a model that included n=1 (individual words), n=2 (pairs), and n=3.  Using n-grams gives us at least *some* characterization of the relative positions of words in each comment. It's no surprise that this improves our ability to predict a comment's origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the subreddit that a comment has come from, or by extension the gender of a comment author, is a **classification problem**.  We want to classify each comment into one of two categories based on the combinations of words it contains.  One of the classic techniques in classification is **Logistic Regression**.  This name may be confusing, because typically when we talk about regression, we are trying to predict a continuous variable, rather than classifying into categories.  The connection is that logistic regression *does* technically generate a continuous variable.  But what it generates is a **predicted probability** value, which represents how likely the model thinks it is that the comment is in a given category.  Whichever category has the higher predicted probability will be the model's prediction for that comment.\n",
    "\n",
    "You may be familiar with the ideas behind **linear regression**, where we try and map a line to data that minimizes the total prediction error across a bunch of observations.  Logistic regression is very similar.  It trains to minimize the error between it's predicted probabilities and the actual answers (0 or 1) for each comment.  \n",
    "\n",
    "need to mention maximum likelihood estimation and logistic function, sigmoid function, maybe a picture...\n",
    "\n",
    "\n",
    "\n",
    "I experimented with two other classification techniques as a part of this project, including **Naive Bayes** and sklearn's **Random Forest Classifier**.  These are entirely different algorithms which I will not discuss further here.  Random Forest, in particular, is a decision-tree based technique that tends to be very powerful.  However in this case, I found that ***Logistic Regression returned the highest accuracy scores***.  More importantly perhaps, logistic regression is the best suited to our goals later in this project, because it is extremely **interpretable**, meaning we can clearly characterize the factors (i.e.; the words) influencing its final predictions. These other methods are also interpretable to a degree, but less so.  This advantage is why logistic regression remains a *very* frequently used technique in the world of classification problems.\n",
    "\n",
    "The last thing we need to know about logistic regression is that it involves **regularization**.  For a thorough explanation of regularization, please see the explanation I have made **[here](LINK THE EXPLANATION PARAGRAPH)**.  But the short version is that it is an extension of Logistic Regression's loss function that counterbalances the attempt to minimize error by also trying to minimize the scale of the feature coefficients.  Sometimes, especially when you have a large number of features like we do here (every word is a feature!), you can over-train to your learning data until you're just training to noise that will be useless for classifying new data.  By restricting the size of coefficients, regularization provides resistance against that happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could talk for days about how to build the best performing model, and there are certainly other improvements we could explore.  But the core process is **GridSearching over hyperparameters**.  We talked above about some of the different hyperparameter options in language processing.  Here are some that we need to consider: \n",
    "\n",
    "- *Should we exclude stop words or not?*\n",
    "- *Do we want to just use single words, or should we look at n-grams too?*\n",
    "- *How many different comments does a word or n-gram need to appear in, for it to be included as a predictor?* \n",
    "- *What type of regularization do we want to use to prevent overfitting?*\n",
    "- *How strong do we want our regularization to be?*\n",
    "\n",
    "Basically, the GridSearch function will fit a model a few times each (splitting the data differently each time) across a bunch of different hyperparameter combinations, and select the hyperparameter combination that returns the highest average accuracy score.\n",
    "\n",
    "In addition, we wanted to try out both vectorizers.  The vectorizer function and the LogisticRegression() function both have hyperparameters that we want to test with.  What we can do is set up a **pipeline**, which will let us run one GridSearch across the hyperparameters for both functions.  We'll do one pipeline for each vectorizer and see what gives the best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Separate data into comment text (features) and subreddit (target variable)\n",
    "X = comments['body']\n",
    "y = comments['subreddit']\n",
    "\n",
    "#Train test split. Stratify=y guarantees that class balance will be maintained across train and test bloc\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,shuffle=True,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch with CountVectorizer and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 24 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of  96 | elapsed: 24.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__ngram_range': [(1, 3)], 'vect__min_df': [2, 5], 'vect__stop_words': [None, 'english'], 'model__penalty': ['l2', 'l1'], 'model__C': [0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('model', LogisticRegression())\n",
    "     ])\n",
    "\n",
    "params = {\n",
    "    'vect__ngram_range':[(1,3)],\n",
    "    'vect__min_df':[2,5],\n",
    "    'vect__stop_words':[None,'english'],\n",
    "    'model__penalty':['l2','l1'],\n",
    "    'model__C':[0.1, 1, 10],\n",
    "}\n",
    "\n",
    "gs_lr1 = GridSearchCV(pipe, params, cv=4, verbose=3, n_jobs=-1)\n",
    "\n",
    "gs_lr1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9105414233396021"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training data accuracy score\n",
    "gs_lr1.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7040798147158382"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test data accuracy score\n",
    "gs_lr1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "          ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "          strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "          tokenizer=None, vocabulary=None)),\n",
       " ('model',\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shows us which hyperparameters were chosen\n",
    "gs_lr1.best_estimator_.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch with TdidfVectorizer and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 24 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of  96 | elapsed: 14.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__ngram_range': [(1, 3)], 'vect__min_df': [2, 5], 'vect__stop_words': [None, 'english'], 'model__penalty': ['l2', 'l1'], 'model__C': [0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2 = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('model', LogisticRegression())\n",
    "     ])\n",
    "\n",
    "params = {\n",
    "    'vect__ngram_range':[(1,3)],\n",
    "    'vect__min_df':[2,5],\n",
    "    'vect__stop_words':[None,'english'],\n",
    "    'model__penalty':['l2','l1'],\n",
    "    'model__C':[0.1, 1, 10],\n",
    "}\n",
    "\n",
    "gs_lr2 = GridSearchCV(pipe2, params, cv=4, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs_lr2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8380481045234089"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training data score\n",
    "gs_lr2.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7051487618029574"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test data score\n",
    "gs_lr2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "          ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "          stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "          token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "          vocabulary=None)),\n",
       " ('model',\n",
       "  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shows which hyperparameters were chosen\n",
    "gs_lr2.best_estimator_.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two vectorizers performed comparably, with accuracy around **70.5%**.  Whenever we attempt a classification problem, we want to keep in mind our **baseline accuracy**, which depends on the class balance of our target variable.  Think of it this way:  if we have 1000 comments, and 900 are from women, the model could simply predict that all comments are from women, and it would have a 90% accuracy score.  In fact, this is exactly what models will often do in such cases.  **This is why we try to balance our classes.**  In this case, we had 36335 AskWomen comments, 31019 AskMen, so the baseline accuracy = 36335/67354, or about **54%**.  Raising this to **70.5%** is a considerable improvement.\n",
    "\n",
    "In evaluating your model's performance, it's also helpful to think about the reality of the challenge.  Predicting gender from text is not trivial. And the majority of these comments are short.  ***What percentage of Reddit comments do we think a human could correctly categorize as male or female, just looking at the text?***  70% seems fairly high. \n",
    "\n",
    "Let's proceed with the TfidfVec model, which has a slight edge in performance.  The final model:\n",
    "\n",
    "- *uses Tfidf Vectorization with normalization*\n",
    "- *uses Ridge regularization with strength $\\alpha$ = 1*\n",
    "- *includes n-grams of length 1, 2, and 3*\n",
    "- *does *not* exclude stop words*\n",
    "- *includes only words or n-grams that appear in at least 5 comments*\n",
    "\n",
    "One thing that causes some concern is that the training scores are noticeably higher than the test scores, which typically indicates overfitting.  This is a bit surprising, because the cross-validation built in to the GridSearch process involves evaluating the model on unseen data.  To investigate this, I tried setting a maximum number of features and running the model again.  What I found is that if I maxed features at 3000, this effect disappeared, and returned in proportion as I raised the max up again.  Still, the model accuracy on the test set marginally improved with the increased features, so I will continue with the model as is.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for LogReg with Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7051487618029574\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     AskMen       0.70      0.63      0.66      7755\n",
      "   AskWomen       0.71      0.77      0.74      9084\n",
      "\n",
      "avg / total       0.70      0.71      0.70     16839\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted AskMen</th>\n",
       "      <th>Predicted AskWomen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual AskMen</th>\n",
       "      <td>4879</td>\n",
       "      <td>2876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual AskWomen</th>\n",
       "      <td>2089</td>\n",
       "      <td>6995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Predicted AskMen  Predicted AskWomen\n",
       "Actual AskMen                4879                2876\n",
       "Actual AskWomen              2089                6995"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fancy_confusion_matrix(y_test, preds):\n",
    "\n",
    "    cmat = confusion_matrix(y_test, preds)\n",
    "    print(f'Accuracy: {accuracy_score(y_test, preds)}')\n",
    "    print(classification_report(y_test, preds))\n",
    "    return pd.DataFrame(cmat, columns=['Predicted ' + str(i) for i in ['AskMen','AskWomen']],\\\n",
    "            index=['Actual ' + str(i) for i in ['AskMen','AskWomen']])\n",
    "\n",
    "predicts = gs_lr2.predict(X_test)\n",
    "fancy_confusion_matrix(y_test, predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(gs_lr2.predict_proba(X))\n",
    "predictions['text'] = comments['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29958</th>\n",
       "      <td>0.995476</td>\n",
       "      <td>0.004524</td>\n",
       "      <td>You need to be able to distinguish between a g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9195</th>\n",
       "      <td>0.990102</td>\n",
       "      <td>0.009898</td>\n",
       "      <td>Start with a physical change. Get a good hairc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46357</th>\n",
       "      <td>0.989392</td>\n",
       "      <td>0.010608</td>\n",
       "      <td>Seyi shay. Her composure!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10730</th>\n",
       "      <td>0.988990</td>\n",
       "      <td>0.011010</td>\n",
       "      <td>She’s my wife :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62159</th>\n",
       "      <td>0.987332</td>\n",
       "      <td>0.012668</td>\n",
       "      <td>My wife walked with her mom.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5575</th>\n",
       "      <td>0.987021</td>\n",
       "      <td>0.012979</td>\n",
       "      <td>Well, most likely if she’s sucking on yo dick ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6819</th>\n",
       "      <td>0.986445</td>\n",
       "      <td>0.013555</td>\n",
       "      <td>&amp;gt;girl &amp;gt;friend Pick one OP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23865</th>\n",
       "      <td>0.985701</td>\n",
       "      <td>0.014299</td>\n",
       "      <td>Usually, you are attracted to the person befor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26589</th>\n",
       "      <td>0.985633</td>\n",
       "      <td>0.014367</td>\n",
       "      <td>You tried... She declined... She knows you tri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20508</th>\n",
       "      <td>0.985479</td>\n",
       "      <td>0.014521</td>\n",
       "      <td>Say nothing, don't answer any questions. Becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11539</th>\n",
       "      <td>0.985391</td>\n",
       "      <td>0.014609</td>\n",
       "      <td>I'm a girl and I would say if you're unsure ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14715</th>\n",
       "      <td>0.985179</td>\n",
       "      <td>0.014821</td>\n",
       "      <td>Dated like a 9.5/10. She was a child model but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20445</th>\n",
       "      <td>0.984321</td>\n",
       "      <td>0.015679</td>\n",
       "      <td>*\"You already tied my hands when when you aske...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24931</th>\n",
       "      <td>0.983710</td>\n",
       "      <td>0.016290</td>\n",
       "      <td>If she's that important to you, ask her if she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5771</th>\n",
       "      <td>0.983663</td>\n",
       "      <td>0.016337</td>\n",
       "      <td>&amp;gt; 5/7 days a week, 2 of which she may get p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16115</th>\n",
       "      <td>0.982606</td>\n",
       "      <td>0.017394</td>\n",
       "      <td>You're correct that you waited too long. Now, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11696</th>\n",
       "      <td>0.982498</td>\n",
       "      <td>0.017502</td>\n",
       "      <td>Let her deal with it. She's your girlfriend ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>0.982411</td>\n",
       "      <td>0.017589</td>\n",
       "      <td>ffs, stop obsessing, start living. I'll be blu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21541</th>\n",
       "      <td>0.982006</td>\n",
       "      <td>0.017994</td>\n",
       "      <td>&amp;gt; I'm a bad guy? How you feel if you live t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11960</th>\n",
       "      <td>0.981889</td>\n",
       "      <td>0.018111</td>\n",
       "      <td>Your GF should be willing to block/unfriend hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7629</th>\n",
       "      <td>0.981193</td>\n",
       "      <td>0.018807</td>\n",
       "      <td>First sight. If she's attractive I'd have sex ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26506</th>\n",
       "      <td>0.979376</td>\n",
       "      <td>0.020624</td>\n",
       "      <td>Know how she takes her coffee or tea, and make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21410</th>\n",
       "      <td>0.979373</td>\n",
       "      <td>0.020627</td>\n",
       "      <td>Cold hard practiced rejection. No...I'm seriou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15474</th>\n",
       "      <td>0.978634</td>\n",
       "      <td>0.021366</td>\n",
       "      <td>&amp;gt;it turns her on to turn you on A good star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15221</th>\n",
       "      <td>0.978353</td>\n",
       "      <td>0.021647</td>\n",
       "      <td>I dont think its breakup worthy yet. Maybe you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1                                               text\n",
       "29958  0.995476  0.004524  You need to be able to distinguish between a g...\n",
       "9195   0.990102  0.009898  Start with a physical change. Get a good hairc...\n",
       "46357  0.989392  0.010608                         Seyi shay. Her composure! \n",
       "10730  0.988990  0.011010                                  She’s my wife :) \n",
       "62159  0.987332  0.012668                      My wife walked with her mom. \n",
       "5575   0.987021  0.012979  Well, most likely if she’s sucking on yo dick ...\n",
       "6819   0.986445  0.013555                    &gt;girl &gt;friend Pick one OP\n",
       "23865  0.985701  0.014299  Usually, you are attracted to the person befor...\n",
       "26589  0.985633  0.014367  You tried... She declined... She knows you tri...\n",
       "20508  0.985479  0.014521  Say nothing, don't answer any questions. Becau...\n",
       "11539  0.985391  0.014609  I'm a girl and I would say if you're unsure ju...\n",
       "14715  0.985179  0.014821  Dated like a 9.5/10. She was a child model but...\n",
       "20445  0.984321  0.015679  *\"You already tied my hands when when you aske...\n",
       "24931  0.983710  0.016290  If she's that important to you, ask her if she...\n",
       "5771   0.983663  0.016337  &gt; 5/7 days a week, 2 of which she may get p...\n",
       "16115  0.982606  0.017394  You're correct that you waited too long. Now, ...\n",
       "11696  0.982498  0.017502  Let her deal with it. She's your girlfriend ri...\n",
       "15995  0.982411  0.017589  ffs, stop obsessing, start living. I'll be blu...\n",
       "21541  0.982006  0.017994  &gt; I'm a bad guy? How you feel if you live t...\n",
       "11960  0.981889  0.018111  Your GF should be willing to block/unfriend hi...\n",
       "7629   0.981193  0.018807  First sight. If she's attractive I'd have sex ...\n",
       "26506  0.979376  0.020624  Know how she takes her coffee or tea, and make...\n",
       "21410  0.979373  0.020627  Cold hard practiced rejection. No...I'm seriou...\n",
       "15474  0.978634  0.021366  &gt;it turns her on to turn you on A good star...\n",
       "15221  0.978353  0.021647  I dont think its breakup worthy yet. Maybe you..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.sort_values(1)[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame(gs_lr2.best_estimator_.steps[1][1].coef_)\n",
    "coefs.columns=gs_lr2.best_estimator_.steps[0][1].get_feature_names()\n",
    "coefs_ranks = \n",
    "coef_ranks.to_csv('coefs_ranks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close examination of the coefficients in the Tfidf model above show coefficients that are reasonably sized, and also very plausible as predictors.  So the models do seem to be working correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Only For Long Comments (>20 Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing I am interested in here is how this will perform if I limit the comments to word length > 20.  It seems likely that the model will be able to be more accurate for full-length comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45194"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longcomments = comments[comments['word_length']>=20].copy()\n",
    "len(longcomments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_long = longcomments['body']\n",
    "y_long = longcomments['subreddit']\n",
    "X_train_long, X_test_long, y_train_long, y_test_long = train_test_split(X_long,y_long,shuffle=True,stratify=y_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 24 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of  96 | elapsed: 13.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__ngram_range': [(1, 3)], 'vect__min_df': [2, 5], 'vect__stop_words': [None, 'english'], 'model__penalty': ['l2', 'l1'], 'model__C': [0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelong = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('model', LogisticRegression())\n",
    "     ])\n",
    "\n",
    "params = {\n",
    "    'vect__ngram_range':[(1,3)],\n",
    "    'vect__min_df':[2,5],\n",
    "    'vect__stop_words':[None,'english'],\n",
    "    'model__penalty':['l2','l1'],\n",
    "    'model__C':[0.1, 1, 10],\n",
    "}\n",
    "\n",
    "gs_lr_long = GridSearchCV(pipelong, params, cv=4, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs_lr_long.fit(X_train_long, y_train_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998465850420416"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_long.score(X_train_long,y_train_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7258164439330914"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_long.score(X_test_long,y_test_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a small improvement in predictive power, but for the most part the numbers mimic those seen in the wider set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions, Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Limitations of AskMen vs AskWomen first-tier replies as a proxy for gender:  \"I'm a girl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Would be cool to look at AskMen comments and see which it most thinks are from a woman, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
