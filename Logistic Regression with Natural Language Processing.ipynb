{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING A MODEL TO PREDICT GENDER FROM TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do We Represent Language So Computers Can Analyze It?\n",
    "\n",
    "If we want to analyze Reddit comments to make some prediction about them, we need to first find a way to **vectorize**, or numerically represent, the text of the comments.  This is the first concern of **natural language processing (NLP)**.  The entire comment collection taken together is referred to as the **corpus**, from the Latin for body.\n",
    "\n",
    "One common approach to vectorizing words is called the **bag of words** representation.  In this strategy, we don't care about the relative positions of any of the words in a comment.  All we do is break the words into units (**tokenization**), count how often a given word appears in each comment, and then create a vector for each comment that is just the number of times each word appears, including all the 0s for words that appear somewhere in the corpus, but don't appear in that particular comment.  \n",
    "When we do this, we usually want to exclude words that are extremely common, like \"the\", \"and\", etc... these words dominate frequencies, but their influence on the meaning of the text is usually minimal.  So it's better to leave them out if we're trying to make accurate predictions.  In NLP, these words are called **stop words**, and there are standard exclusion lists that are built into the available vectorizers as options.\n",
    "\n",
    "There are a few different vectorizers to choose from:  **CountVectorizer()** and **TfidfVectorizer()** are the most widely used.  CountVectorizer tokenizes and counts, and that's it.  TfIdfVectorizer goes a step further, and **normalizes** the frequencies.  Basically, TfidfVectorizer will apply a transformation to our comment vectors that will **down-weight the influence of words that appear in a lot of comments, while up-weighting the influence of rarer words**.  The idea behind this is that common words tend to be less interesting to us; they give us less information about the comment.  Rarer words are more likely to have predictive value. \n",
    "\n",
    "That said, if a word is *too rare*, and only appears in one or two comments, there isn't much point to including it - what patterns can it really give us?  There are parameters that will allow us to only include words in our analysis if they appear in at least *n* comments.  \n",
    "\n",
    "Single words may be able to help us predict whether a comment is AskMen or AskWomen, but what if we want to consider the frequency of certain *pairs* of words, or *triads*?  This is where **n-grams**, or groups of n words, come into play.  We can run our model with different combinations of n-grams included and see which performs best.  In this case, we found the best performance with a model that included n=1 (individual words), n=2 (pairs), and n=3.  Using n-grams gives us at least *some* characterization of the relative positions of words in each comment. It's no surprise that this improves our ability to predict a comment's origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression: Classification We Can Explain\n",
    "\n",
    "Predicting the subreddit that a comment has come from, or by extension the gender of a comment author, is a **classification problem**.  We want to classify each comment into one of two categories based on the combinations of words it contains.  \n",
    "\n",
    "One of the classic techniques in classification is **logistic regression**.  This name may be confusing, because typically when we talk about regression, we are trying to predict a continuous variable, rather than classifying into categories.  The connection is that logistic regression *does* technically output a continuous variable.  But what it outputs is a **predicted probability**, which represents how likely the model thinks it is that the comment is in a given category. \n",
    "\n",
    "Logistic regression derives its name from the **logistic function**, which maps numbers in the range $(-\\infty, \\infty)$ to the range $(0,1)$. To generate a probability output from a linear combination of features and regression coefficients, logistic regression needs to performs this mapping effect, always returning outputs in the $(0,1)$ range.  You may be familiar with the ideas behind **linear regression**, in which a line is fit to data such that it minimizes the total prediction error across all observations.  Logistic regression uses a different optimization method than the least squares technique used in linear regression, but it is similar in that it ultimately relies on optimizing regression coefficients for each feature variable, which makes it similarly very interpretable, and that it seeks to ***minimize the overall error between its predicted probabilities and the true answers (0 or 1) for each comment***. \n",
    "<img src=\"images/logreg.png\">\n",
    "I experimented with two other classification techniques as a part of this project, including **Naive Bayes** and sklearn's **Random Forest Classifier**.  These are entirely different algorithms which I will not discuss further here.  Random Forest, in particular, is a decision-tree based technique that tends to be very powerful.  However in this case, I found that ***logistic regression actually returned the highest accuracy scores***.  \n",
    "\n",
    "More importantly though, logistic regression is the best approach for our goals in this project, because it is extremely **interpretable**, meaning we can clearly characterize the factors (i.e.; the words or n-grams) influencing its final predictions. The other methods I've mentioned are also interpretable to a degree, but less so.  This explanatory advantage is why logistic regression remains a *very* frequently used technique in the world of classification problems.\n",
    "\n",
    "The last thing we need to know about logistic regression is that it involves **regularization**.  For a thorough explanation of regularization, please see the explanation I have made **[here](../projects/housing-regression.html#Feature-Elimination-and-Regularization)**.  But the short version is that it is an extension of Logistic Regression's loss function that counterbalances the attempt to minimize error by also trying to minimize the scale of the feature coefficients.  Sometimes, especially when you have a large number of features like we do here (every word is a feature!), you can over-train to your learning data until you're just training to noise that will be useless for classifying new data.  By restricting the size of coefficients, regularization provides resistance against this happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Best Model\n",
    "\n",
    "We could talk for days about how to build the best performing model, and there are certainly other improvements we could explore.  But the core process is **GridSearching over hyperparameters**.  We talked above about some of the different hyperparameter options in language processing.  Here are some that we need to consider: \n",
    "\n",
    "- *Should we exclude stop words or not?*\n",
    "- *Do we want to just use single words, or should we look at n-grams too?*\n",
    "- *How many different comments does a word or n-gram need to appear in, for it to be included as a predictor?* \n",
    "- *What type of regularization do we want to use to prevent overfitting?*\n",
    "- *How strong do we want our regularization to be?*\n",
    "\n",
    "Basically, the GridSearch function will fit a model a few times each (splitting the data differently each time) across a bunch of different hyperparameter combinations, and select the hyperparameter combination that returns the highest average accuracy score.\n",
    "\n",
    "In addition, we wanted to try out both vectorizers.  The vectorizer function and the LogisticRegression() function both have hyperparameters that we want to test with.  What we can do is set up a **pipeline**, which will let us run one GridSearch across the hyperparameters for both functions.  We'll do one pipeline for each vectorizer and see what gives the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate data into comment text (features) and subreddit (target variable)\n",
    "X = comments['body']\n",
    "y = comments['subreddit']\n",
    "\n",
    "#Train test split. Stratify=y guarantees that class balance will be maintained across train and test bloc\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,shuffle=True,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch: CountVectorizer and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 24 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of  96 | elapsed: 24.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__ngram_range': [(1, 3)], 'vect__min_df': [2, 5], 'vect__stop_words': [None, 'english'], 'model__penalty': ['l2', 'l1'], 'model__C': [0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creates pipeline to handle vectorization and logistic regression steps\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('model', LogisticRegression())\n",
    "     ])\n",
    "\n",
    "#Specifies different hyperparameter values that we want to test across\n",
    "params = {\n",
    "    'vect__ngram_range':[(1,3)],\n",
    "    'vect__min_df':[2,5],\n",
    "    'vect__stop_words':[None,'english'],\n",
    "    'model__penalty':['l2','l1'],\n",
    "    'model__C':[0.1, 1, 10],\n",
    "}\n",
    "\n",
    "#Executes GridSearch\n",
    "gs_lr1 = GridSearchCV(pipe, params, cv=4, verbose=3, n_jobs=-1)\n",
    "gs_lr1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9105414233396021"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training data accuracy score\n",
    "gs_lr1.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7040798147158382"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test data accuracy score\n",
    "gs_lr1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "          ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "          strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "          tokenizer=None, vocabulary=None)),\n",
       " ('model',\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shows us which hyperparameters were chosen\n",
    "gs_lr1.best_estimator_.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch: TdidfVectorizer and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 24 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of  96 | elapsed: 14.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__ngram_range': [(1, 3)], 'vect__min_df': [2, 5], 'vect__stop_words': [None, 'english'], 'model__penalty': ['l2', 'l1'], 'model__C': [0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2 = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('model', LogisticRegression())\n",
    "     ])\n",
    "\n",
    "params = {\n",
    "    'vect__ngram_range':[(1,3)],\n",
    "    'vect__min_df':[2,5],\n",
    "    'vect__stop_words':[None,'english'],\n",
    "    'model__penalty':['l2','l1'],\n",
    "    'model__C':[0.1, 1, 10],\n",
    "}\n",
    "\n",
    "gs_lr2 = GridSearchCV(pipe2, params, cv=4, verbose=2, n_jobs=-1)\n",
    "gs_lr2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8380481045234089"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training data score\n",
    "gs_lr2.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7051487618029574"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test data score\n",
    "gs_lr2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "          ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "          stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "          token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "          vocabulary=None)),\n",
       " ('model',\n",
       "  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shows which hyperparameters were chosen\n",
    "gs_lr2.best_estimator_.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Results\n",
    "\n",
    "The two vectorizers performed comparably, with accuracy around **70.5%**.  Whenever we attempt a classification problem, we want to keep in mind our **baseline accuracy**, which depends on the class balance of our target variable.  Think of it this way:  if we have 1000 comments, and 900 are from women, the model could simply predict that all comments are from women, and it would have a 90% accuracy score.  In fact, this is exactly what models will often do in such cases.  **This is why we try to balance our classes.**  In this case, we had 36335 AskWomen comments, 31019 AskMen, so the baseline accuracy = 36335/67354, or about **54%**.  Raising this to **70.5%** is a considerable improvement.\n",
    "\n",
    "In evaluating your model's performance, it's also helpful to think about the reality of the challenge.  Predicting gender from text is not trivial. And the majority of these comments are short.  ***What percentage of Reddit comments do we think a human could correctly categorize as male or female, just looking at the text?***  70% seems fairly high. \n",
    "\n",
    "Let's proceed with the TfidfVec model, which has a slight edge in performance.  The final model:\n",
    "\n",
    "- *uses Tfidf Vectorization with normalization*\n",
    "- *uses Ridge regularization with strength $\\alpha$ = 1*\n",
    "- *includes n-grams of length 1, 2, and 3*\n",
    "- *does *not* exclude stop words*\n",
    "- *includes only words or n-grams that appear in at least 5 comments*\n",
    "\n",
    "One thing that causes some concern is that the training scores are noticeably higher than the test scores, which typically indicates overfitting.  This is a bit surprising, because the cross-validation built in to the GridSearch process involves evaluating the model on unseen data.  To investigate this, I tried setting a maximum number of features and running the model again.  What I found is that if I maxed features at 3000, this effect disappeared, and returned in proportion as I raised the max up again.  Still, the model accuracy on the test set marginally improved with the increased features, so I will continue with the model as is.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for LogReg with Tfidf\n",
    "\n",
    "A confusion matrix gives us a clearer sense of where our model is performing better or worse.  See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7051487618029574\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     AskMen       0.70      0.63      0.66      7755\n",
      "   AskWomen       0.71      0.77      0.74      9084\n",
      "\n",
      "avg / total       0.70      0.71      0.70     16839\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted AskMen</th>\n",
       "      <th>Predicted AskWomen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual AskMen</th>\n",
       "      <td>4879</td>\n",
       "      <td>2876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual AskWomen</th>\n",
       "      <td>2089</td>\n",
       "      <td>6995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Predicted AskMen  Predicted AskWomen\n",
       "Actual AskMen                4879                2876\n",
       "Actual AskWomen              2089                6995"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fancy_confusion_matrix(y_test, preds):\n",
    "\n",
    "    cmat = confusion_matrix(y_test, preds)\n",
    "    print(f'Accuracy: {accuracy_score(y_test, preds)}')\n",
    "    print(classification_report(y_test, preds))\n",
    "    return pd.DataFrame(cmat, columns=['Predicted ' + str(i) for i in ['AskMen','AskWomen']],\\\n",
    "            index=['Actual ' + str(i) for i in ['AskMen','AskWomen']])\n",
    "\n",
    "predicts = gs_lr2.predict(X_test)\n",
    "fancy_confusion_matrix(y_test, predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, when the model makes a prediction, the **precision** of that prediction, i.e.; the percentage of times the prediction is correct, is about the same, regardless of what was predicted (~70-71%).  However, the **recall**, or **sensitivity**, varies significantly between the genders.  The model has a harder time detecting that an AskMen comment comes from AskMen (63%) than it does detecting that an AskWomen comment comes from AskWomen (77%).  Basically, this suggests that ***the word or n-gram signature of female comments is a bit more distinctive***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOW FOR THE FUN STUFF...\n",
    "\n",
    "### Identifying the Most Gender-Predictive Words and Phrases\n",
    "\n",
    "The first thing we can do is look at the regression coefficients for every word or n-gram, sorted negative to positive.  In this model, we have assigned AskMen comments to 0 and AskWomen comments to 1.  So the words or n-grams with the strongest *positive* coefficients in our model are the words or n-grams that are most predictive of female authorship (they push the outcome towards 1 the most).  And the words or n-grams with the strongest *negative* coefficients are the most predictive of male authorship (they push the outcome toward 0).  This is how we can ***identify the words and n-grams that point towards a gender.*** And REMEMBER - the words with the most influential coefficients one way or the other don't just represent the words that are most likely to appear in comments by a given gender. They also have to be *unlikely to appear* in comments made by the *other gender*.  This is a binary classification problem, after all. In short, ***the model is looking for words and n-grams that are highly frequent in one subreddit RELATIVE TO THE OTHER.***\n",
    "\n",
    "By sorting the resulting coefficients and looking at the 200 words and n-grams that are most predictive of each gender's authorship, we will see that obvious gender-oriented and subject-oriented keywords and phrases are predominant.  But we can also begin to notice subtler linguistic trends, which I have summarized in the presentation slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame(gs_lr2.best_estimator_.steps[1][1].coef_).T\n",
    "coefs.columns = ['coef']\n",
    "coefs['ngram'] = gs_lr2.best_estimator_.steps[0][1].get_feature_names()\n",
    "coefs = coefs[['ngram','coef']]\n",
    "coefs = coefs.sort_values('coef')\n",
    "coefs.to_csv('./csvs/coef_ranks.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wife</td>\n",
       "      <td>-10.541040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>girlfriend</td>\n",
       "      <td>-9.225318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my wife</td>\n",
       "      <td>-8.849548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gf</td>\n",
       "      <td>-8.825405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>girls</td>\n",
       "      <td>-8.262061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>beard</td>\n",
       "      <td>-8.193986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>girl</td>\n",
       "      <td>-7.893103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>my girlfriend</td>\n",
       "      <td>-7.086567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>her</td>\n",
       "      <td>-6.864349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>you</td>\n",
       "      <td>-6.782092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dick</td>\n",
       "      <td>-6.748809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>father</td>\n",
       "      <td>-6.585669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>she</td>\n",
       "      <td>-6.401139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>if she</td>\n",
       "      <td>-6.349993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gt</td>\n",
       "      <td>-6.295696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ngram       coef\n",
       "0            wife -10.541040\n",
       "1      girlfriend  -9.225318\n",
       "2         my wife  -8.849548\n",
       "3              gf  -8.825405\n",
       "4           girls  -8.262061\n",
       "5           beard  -8.193986\n",
       "6            girl  -7.893103\n",
       "7   my girlfriend  -7.086567\n",
       "8             her  -6.864349\n",
       "9             you  -6.782092\n",
       "10           dick  -6.748809\n",
       "11         father  -6.585669\n",
       "12            she  -6.401139\n",
       "13         if she  -6.349993\n",
       "14             gt  -6.295696"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shows words and n-grams most strongly associated with comments by men\n",
    "coefs.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421776"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have over 400,000 different words and n-grams in our feature list!  Let's take the **200 most predictive words/n-grams for each gender** (top .0005) and make word clouds to visualize them.\n",
    "\n",
    "### Gender-Predictive Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "AskMenWordCloud = coefs[0:200]\n",
    "AskWomenWordCloud = coefs.sort_values('coef',ascending=False)[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to create a wordcloud:\n",
    "d={}\n",
    "for key, val in AskMenWordCloud.values:\n",
    "    d[key] = int(val**2)\n",
    "    \n",
    "wordcloud = WordCloud(width = 2000, height = 2000, \n",
    "                background_color ='white',  \n",
    "                min_font_size = 10).generate_from_frequencies(d)\n",
    "\n",
    "plt.figure(figsize = (20, 20), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the Most Gender-Linked Comments\n",
    "\n",
    "Each comment in our data set has a predicted probability of being male or female-authored attached to it.  So another interesting thing we can do is sort by these predicted probabilities to identify ***the comments that the model identifies as the* most *obviously male or* most  *obviously female***.  Let's take a look at some of the top results:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Puts predicted probabilities and associated text into a dataframe\n",
    "predictions = pd.DataFrame(gs_lr2.predict_proba(X))\n",
    "predictions['text'] = comments['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29958</th>\n",
       "      <td>0.995476</td>\n",
       "      <td>0.004524</td>\n",
       "      <td>You need to be able to distinguish between a g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9195</th>\n",
       "      <td>0.990102</td>\n",
       "      <td>0.009898</td>\n",
       "      <td>Start with a physical change. Get a good hairc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46357</th>\n",
       "      <td>0.989392</td>\n",
       "      <td>0.010608</td>\n",
       "      <td>Seyi shay. Her composure!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10730</th>\n",
       "      <td>0.988990</td>\n",
       "      <td>0.011010</td>\n",
       "      <td>Sheâ€™s my wife :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62159</th>\n",
       "      <td>0.987332</td>\n",
       "      <td>0.012668</td>\n",
       "      <td>My wife walked with her mom.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5575</th>\n",
       "      <td>0.987021</td>\n",
       "      <td>0.012979</td>\n",
       "      <td>Well, most likely if sheâ€™s sucking on yo dick ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6819</th>\n",
       "      <td>0.986445</td>\n",
       "      <td>0.013555</td>\n",
       "      <td>&amp;gt;girl &amp;gt;friend Pick one OP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23865</th>\n",
       "      <td>0.985701</td>\n",
       "      <td>0.014299</td>\n",
       "      <td>Usually, you are attracted to the person befor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26589</th>\n",
       "      <td>0.985633</td>\n",
       "      <td>0.014367</td>\n",
       "      <td>You tried... She declined... She knows you tri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20508</th>\n",
       "      <td>0.985479</td>\n",
       "      <td>0.014521</td>\n",
       "      <td>Say nothing, don't answer any questions. Becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11539</th>\n",
       "      <td>0.985391</td>\n",
       "      <td>0.014609</td>\n",
       "      <td>I'm a girl and I would say if you're unsure ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14715</th>\n",
       "      <td>0.985179</td>\n",
       "      <td>0.014821</td>\n",
       "      <td>Dated like a 9.5/10. She was a child model but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20445</th>\n",
       "      <td>0.984321</td>\n",
       "      <td>0.015679</td>\n",
       "      <td>*\"You already tied my hands when when you aske...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24931</th>\n",
       "      <td>0.983710</td>\n",
       "      <td>0.016290</td>\n",
       "      <td>If she's that important to you, ask her if she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5771</th>\n",
       "      <td>0.983663</td>\n",
       "      <td>0.016337</td>\n",
       "      <td>&amp;gt; 5/7 days a week, 2 of which she may get p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16115</th>\n",
       "      <td>0.982606</td>\n",
       "      <td>0.017394</td>\n",
       "      <td>You're correct that you waited too long. Now, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11696</th>\n",
       "      <td>0.982498</td>\n",
       "      <td>0.017502</td>\n",
       "      <td>Let her deal with it. She's your girlfriend ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>0.982411</td>\n",
       "      <td>0.017589</td>\n",
       "      <td>ffs, stop obsessing, start living. I'll be blu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21541</th>\n",
       "      <td>0.982006</td>\n",
       "      <td>0.017994</td>\n",
       "      <td>&amp;gt; I'm a bad guy? How you feel if you live t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11960</th>\n",
       "      <td>0.981889</td>\n",
       "      <td>0.018111</td>\n",
       "      <td>Your GF should be willing to block/unfriend hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7629</th>\n",
       "      <td>0.981193</td>\n",
       "      <td>0.018807</td>\n",
       "      <td>First sight. If she's attractive I'd have sex ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26506</th>\n",
       "      <td>0.979376</td>\n",
       "      <td>0.020624</td>\n",
       "      <td>Know how she takes her coffee or tea, and make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21410</th>\n",
       "      <td>0.979373</td>\n",
       "      <td>0.020627</td>\n",
       "      <td>Cold hard practiced rejection. No...I'm seriou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15474</th>\n",
       "      <td>0.978634</td>\n",
       "      <td>0.021366</td>\n",
       "      <td>&amp;gt;it turns her on to turn you on A good star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15221</th>\n",
       "      <td>0.978353</td>\n",
       "      <td>0.021647</td>\n",
       "      <td>I dont think its breakup worthy yet. Maybe you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1                                               text\n",
       "29958  0.995476  0.004524  You need to be able to distinguish between a g...\n",
       "9195   0.990102  0.009898  Start with a physical change. Get a good hairc...\n",
       "46357  0.989392  0.010608                         Seyi shay. Her composure! \n",
       "10730  0.988990  0.011010                                  Sheâ€™s my wife :) \n",
       "62159  0.987332  0.012668                      My wife walked with her mom. \n",
       "5575   0.987021  0.012979  Well, most likely if sheâ€™s sucking on yo dick ...\n",
       "6819   0.986445  0.013555                    &gt;girl &gt;friend Pick one OP\n",
       "23865  0.985701  0.014299  Usually, you are attracted to the person befor...\n",
       "26589  0.985633  0.014367  You tried... She declined... She knows you tri...\n",
       "20508  0.985479  0.014521  Say nothing, don't answer any questions. Becau...\n",
       "11539  0.985391  0.014609  I'm a girl and I would say if you're unsure ju...\n",
       "14715  0.985179  0.014821  Dated like a 9.5/10. She was a child model but...\n",
       "20445  0.984321  0.015679  *\"You already tied my hands when when you aske...\n",
       "24931  0.983710  0.016290  If she's that important to you, ask her if she...\n",
       "5771   0.983663  0.016337  &gt; 5/7 days a week, 2 of which she may get p...\n",
       "16115  0.982606  0.017394  You're correct that you waited too long. Now, ...\n",
       "11696  0.982498  0.017502  Let her deal with it. She's your girlfriend ri...\n",
       "15995  0.982411  0.017589  ffs, stop obsessing, start living. I'll be blu...\n",
       "21541  0.982006  0.017994  &gt; I'm a bad guy? How you feel if you live t...\n",
       "11960  0.981889  0.018111  Your GF should be willing to block/unfriend hi...\n",
       "7629   0.981193  0.018807  First sight. If she's attractive I'd have sex ...\n",
       "26506  0.979376  0.020624  Know how she takes her coffee or tea, and make...\n",
       "21410  0.979373  0.020627  Cold hard practiced rejection. No...I'm seriou...\n",
       "15474  0.978634  0.021366  &gt;it turns her on to turn you on A good star...\n",
       "15221  0.978353  0.021647  I dont think its breakup worthy yet. Maybe you..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shows the top AskMen predicted comments\n",
    "predictions.sort_values(1)[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set String Search\n",
    "\n",
    "Here I built out a quick function to return any comment in my dataset containing a given word or phrase.  This helped me to review full comments of interest after looking at the predict_proba dataframe display.  Could be a useful tool in other contexts as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_comment_if_string(string,comment):\n",
    "    if string.lower() in comment.lower():\n",
    "        print(comment)\n",
    "        print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let her deal with it. She's your girlfriend right? You trust her right? You care about her? She's asking you to let her handle it. So let her handle it. It sucks and it's gonna stress you out probably, but the other dude wins if you fly off the handle or fuck with this dude. Because then GF is mad at you. I would let her deal with it and just be mad a few days. It's okay to be mad, it's understandable.\n",
      "-----\n",
      "I'd do nothing. Let her deal with it. My gf would tell me, instantly block him and move on. If you try to go beat him up or something it does nothing but start more shit.\n",
      "-----\n",
      "Honestly mate.... leave. I was with a woman for about 2 years who suffered with depression. I kept thinking that things would get better and I tried everything to help and support her... but ultimately, she just wasn't coming to the party to try and improve the situation. She wouldn't commit to her medication, she refused therapy, she wouldn't exercise or get out of the house. I was quite easy on her at first and let her deal with it in her own way, gave her plenty of time/space and I remained supportive and understanding. Then I realized just how much I wasn't enjoying life by being with her and living in this dark space. Quite frankly, it was shit. Once i left it was like dropping a heavy bag off rocks and I've never looked back. Life has been great since. May or may not be the answer you seek, but I'd strongly advise it particularly if she's making minimal or no effort to better the situation. You deserve a better quality of life. \n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "#Just change the string and run the loop to search for comments\n",
    "for i in comments['body']:\n",
    "    print_comment_if_string('let her deal',i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where Does the Model Get it Wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at comments where **the true answer does not match the model's prediction**.  Because we have predicted probabilities, we can see exactly how sure the model was about its wrong predictions.  Mostly we would expect that the predicted probabilities would be closer to 0.5/0.5 for wrong answers, but by looking at where the model is highly certain about an *incorrect* answer, we can learn something about our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(gs_lr2.predict_proba(X))\n",
    "predictions['text'] = comments['body']\n",
    "predictions['true'] = comments['subreddit']\n",
    "predictions['pred'] = gs_lr2.predict(X)\n",
    "predictions.columns = ['AskMen','AskWomen','text','true','pred']\n",
    "predictions = predictions[['text','true','pred','AskMen','AskWomen']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at comments where the model guessed wrong.\n",
    "wrong = predictions[predictions['true']!=predictions['pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "      <th>AskMen</th>\n",
       "      <th>AskWomen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62159</th>\n",
       "      <td>My wife walked with her mom.</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>0.998622</td>\n",
       "      <td>0.001378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35101</th>\n",
       "      <td>Youâ€™re pretty. I want you to meet my wife. ðŸ˜³</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>0.996958</td>\n",
       "      <td>0.003042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50655</th>\n",
       "      <td>On the fourth date, talk about a kid you know ...</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>0.990759</td>\n",
       "      <td>0.009241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62856</th>\n",
       "      <td>Girl suggested we go to her room because she w...</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>0.988693</td>\n",
       "      <td>0.011307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60523</th>\n",
       "      <td>Don't attack her. Don't touch her. Don't chang...</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>0.986191</td>\n",
       "      <td>0.013809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54460</th>\n",
       "      <td>Putting a dick in my mouth I thought if you to...</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>0.984748</td>\n",
       "      <td>0.015252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32986</th>\n",
       "      <td>dO yOu EvEn LiFt BrO?</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>0.984536</td>\n",
       "      <td>0.015464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57491</th>\n",
       "      <td>i hang out with her more, show her that she is...</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>0.983074</td>\n",
       "      <td>0.016926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47544</th>\n",
       "      <td>I got a motorcycle.</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>0.980910</td>\n",
       "      <td>0.019090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43611</th>\n",
       "      <td>Off the top of my head, Louis Zamperiniâ€™s stor...</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>0.979093</td>\n",
       "      <td>0.020907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text      true    pred  \\\n",
       "62159                      My wife walked with her mom.   AskWomen  AskMen   \n",
       "35101       Youâ€™re pretty. I want you to meet my wife. ðŸ˜³  AskWomen  AskMen   \n",
       "50655  On the fourth date, talk about a kid you know ...  AskWomen  AskMen   \n",
       "62856  Girl suggested we go to her room because she w...  AskWomen  AskMen   \n",
       "60523  Don't attack her. Don't touch her. Don't chang...  AskWomen  AskMen   \n",
       "54460  Putting a dick in my mouth I thought if you to...  AskWomen  AskMen   \n",
       "32986                              dO yOu EvEn LiFt BrO?  AskWomen  AskMen   \n",
       "57491  i hang out with her more, show her that she is...  AskWomen  AskMen   \n",
       "47544                               I got a motorcycle.   AskWomen  AskMen   \n",
       "43611  Off the top of my head, Louis Zamperiniâ€™s stor...  AskWomen  AskMen   \n",
       "\n",
       "         AskMen  AskWomen  \n",
       "62159  0.998622  0.001378  \n",
       "35101  0.996958  0.003042  \n",
       "50655  0.990759  0.009241  \n",
       "62856  0.988693  0.011307  \n",
       "60523  0.986191  0.013809  \n",
       "54460  0.984748  0.015252  \n",
       "32986  0.984536  0.015464  \n",
       "57491  0.983074  0.016926  \n",
       "47544  0.980910  0.019090  \n",
       "43611  0.979093  0.020907  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicts AskMen strongly when the answer is AskWomen\n",
    "wrong.sort_values('AskMen',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "      <th>AskMen</th>\n",
       "      <th>AskWomen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>My childhood dogs weren't really the best but ...</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>0.032425</td>\n",
       "      <td>0.967575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>Whatever gives me ranged and/or stealthy attacks.</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>0.032550</td>\n",
       "      <td>0.967450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25054</th>\n",
       "      <td>TLDR at the end. We threw a party during the h...</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>0.033550</td>\n",
       "      <td>0.966450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16321</th>\n",
       "      <td>My parents. They got married because my dad go...</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>0.033714</td>\n",
       "      <td>0.966286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21757</th>\n",
       "      <td>My hair. I had really long hair from as long a...</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>0.034704</td>\n",
       "      <td>0.965296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16851</th>\n",
       "      <td>I used to think I was a 6 or 7 but then I star...</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>0.034812</td>\n",
       "      <td>0.965188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28535</th>\n",
       "      <td>I was the SO that fell out of love with my ex....</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>0.036968</td>\n",
       "      <td>0.963032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2751</th>\n",
       "      <td>My hunky and chunky thighs. They feel like steel</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>0.037936</td>\n",
       "      <td>0.962064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27625</th>\n",
       "      <td>He has been nothing but helpful through my anx...</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>AskWomen</td>\n",
       "      <td>0.038056</td>\n",
       "      <td>0.961944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text    true      pred  \\\n",
       "2831   My childhood dogs weren't really the best but ...  AskMen  AskWomen   \n",
       "666    Whatever gives me ranged and/or stealthy attacks.  AskMen  AskWomen   \n",
       "25054  TLDR at the end. We threw a party during the h...  AskMen  AskWomen   \n",
       "16321  My parents. They got married because my dad go...  AskMen  AskWomen   \n",
       "21757  My hair. I had really long hair from as long a...  AskMen  AskWomen   \n",
       "16851  I used to think I was a 6 or 7 but then I star...  AskMen  AskWomen   \n",
       "28535  I was the SO that fell out of love with my ex....  AskMen  AskWomen   \n",
       "2751    My hunky and chunky thighs. They feel like steel  AskMen  AskWomen   \n",
       "27625  He has been nothing but helpful through my anx...  AskMen  AskWomen   \n",
       "\n",
       "         AskMen  AskWomen  \n",
       "2831   0.032425  0.967575  \n",
       "666    0.032550  0.967450  \n",
       "25054  0.033550  0.966450  \n",
       "16321  0.033714  0.966286  \n",
       "21757  0.034704  0.965296  \n",
       "16851  0.034812  0.965188  \n",
       "28535  0.036968  0.963032  \n",
       "2751   0.037936  0.962064  \n",
       "27625  0.038056  0.961944  "
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicts AskWomen strongly when the answer is AskMen\n",
    "wrong.sort_values('AskWomen',ascending=False)[31:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmapping Text by Gender-Association in Tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, in looking at comments that have received very strong gender predictions from the model, one is left wondering what the model is seeing.  The maleness or femaleness of such a comment is usually pretty self-evident, so we have faith that the model is basing its decision on more than just noise. ***Wouldn't it be great if we could take a comment like this, and visualize what the model is identifying as male or female, just by a glance at the text?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "coefs = pd.read_csv('./csvs/coef_ranks.csv')\n",
    "\n",
    "#Copy comment string in here, insert \\ before apostrophes\n",
    "comment = 'You need to be able to distinguish between a girl that likes the attention youre giving her from a girl that likes you. Most girls at a party will smile and talk to you if you approach them with confidence. After you talk to her though, you need to find a way for her to invest in the interaction. If she never invests in it you never really know what she wants. Walking away from the girl without getting any info is a good way to do this. You started the first conversation, now shes on the spot. If she doesnt come up to you and talk to you again, she isnt getting your number. This forces her to be more than just a receiver of attention. If youre leaving the party, or think you might not see her again at that party. Give her your contact info, dont take any of hers. Shes then forced to reach out to you if she wants things to go anywhere. Not only will this tell you if shes actually interested in you, it will make her more attracted. She will see that you arent desperate, and that makes you more attractive. Girls can sense when the only reason youre talking to them is to get their number or something like that. They can see right through you. When you walk away without the number, she realizes youre not just trying to get her number, you actually just wanted to talk to her. That is attractive'\n",
    "\n",
    "#Converts comment to list of lowercase tokens\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "comment_tokens = tokenizer.tokenize(comment.lower())\n",
    "\n",
    "#Creates dataframe with words from comment, in order, with their coefficients\n",
    "coef_list = []\n",
    "for i in range(len(comment_tokens)):\n",
    "    gram = comment_tokens[i]\n",
    "    try:\n",
    "        coef = coefs[coefs['ngram']==gram]['coef'].values[0]\n",
    "    except:\n",
    "        coef = np.nan\n",
    "    coef_list.append(coef)   \n",
    "\n",
    "d = {'gram':comment_tokens,'coef':coef_list}\n",
    "\n",
    "df = pd.DataFrame(d).fillna(0)\n",
    "df.to_csv('./csvs/tableau_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this in Tableau to create a divergent-color text heatmap of the comment.  We use the ngram column as a dimension on text, and the coefficient column as a measure on color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/text-heatmap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Limitations\n",
    "\n",
    "### Additional Directions\n",
    "\n",
    "I tried looking at longer comments only (word length > 20), to see how this affected performance.  About 45000 comments met criteria, and classes remained balanced.  After a full GridSearch, the final model had an accuracy score close to 73%, a moderate but definitive improvement.  This isn't surprising, as the shortest comments should be among the hardest to classify.  But it was interesting to see this confirmed, because it suggests that longer text documents may be gender-identifiable with even higher accuracy, at least up to some plateau.\n",
    "\n",
    "I also tried predicting whether a comment had over or under 10 upvotes, as a binary classification problem, based on the words and n-grams in the comment. This was interesting in the sense that it might show words to avoid or words to include to try and get upvotes when commenting in one subreddit or the other.  Only about 15% of comments have 10 or more upvotes, so there was a class balance problem.  Bootstrapping up resulted in heavily overfit data, to the point of meaninglessness.  Downsampling regular comments basically introduced bias, basically reducing me to data sets of about 9000 comments.  \n",
    "\n",
    "I did see *some* success with this.  In AskMen, I could only really raise accuracy from 50% to around 54%. But in AskWomen, I was able to raise accuracy over 60%, and the confusion matrix and sensitivity scores looked very reasonable.  Still, I felt that the top coefficient factors were rarely plausible, often dominated by stop words.  My ultimate takeaway was that this was largely grasping at straws, which makes sense - most of upvote score is down to high-level context, not individual words.  I did see modest improvement when I added a gap, and looked at 10+ upvote comments against comments with 1 or fewer upvotes.  This suggests to me that it might be worth going back and attacking the question with regression, using upvotes as a continuous target variable, and preferably with a substantially larger data set.\n",
    "\n",
    "\n",
    "### Limitations of First-Tier Comments in AskMen and AskWomen as a Proxy for Gender.\n",
    "\n",
    "Of course, there will be cross-over between the subreddits to a minor degree, with the occasional man responding to an AskWomen question, and vice versa, so a small percentage of our labels will be incorrect.  But there are other limitations to our model.\n",
    "\n",
    "Some of the major shortcomings of the model are the result of bias toward the majority and stereotyping. Homosexuality is one example.  Gay men writing about \"my husband\", \"my boyfriend\", will be classed as female authors, often with very high certainty.  Women who talk about riding a motorcycle, or who swear aggressively, etc etc, will be classed as male authors.  These examples speak to very valid ethical concerns.  Natural language models trained on large corpora often struggle to avoid biases like these.  We need to be careful about the ways in which we employ models that may reinforce stereotyped perceptions and behaviors. \n",
    "\n",
    "The nature of the subreddits themselves can actually create flaws in our gender proxy. To illustrate this, consider the 3-gram **\"I'm a girl\"** or **\"I'm a woman\"**.  You might think this would be a slam dunk for the model, but in fact, this phrase is much more likely to be said by someone (a woman) replying to a question *in AskMen*.  So the model see's \"I'm a girl\" and correctly predicts AskMen, and because we're using AskMen as a proxy for gender, \"I'm a girl\" ends up ranking among the factors that we name as indicating male authorship.  In this case, the mistake is obvious, but there may be other mistakes created by the pretenses of the subreddits that are harder to spot.   \n",
    "\n",
    "I suspect the largest issue is that the subject matter discussed in these subreddits, which is probably often gender-specific, makes the task a bit easier for the model than if we just had a bunch of gender-labeled comments talking about anything and everything.  It also probably throws off what the true most predictive words and ngrams are, because it's more likely that women, for instance, are using the word bra, or husband, or cramps, in this context than in general discussion in other subreddits. I'd be curious to see how the model performs on a gender-labeled data set of comments from all across Reddit, especially to see which word and ngram features maintain or strengthen their influence. \n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "In spite of these limitations, the model definitely brings value.  There were certainly examples, in looking at comments where the model had predicted gender \"incorrectly,\" where the author was in fact a woman posting in AskMen, or a man posting in AskWomen, flagged by the model based on its learning.  It's fair to say that the prevalence of gender-oriented subject matter in these subreddits gave the model extra firepower.  But a surprisingly large number of its leading gender predictors were low-level linguistic signatures that we would expect to be pretty much unconscious.  So I think there *is* something to the idea that men and women tend towards identifiable linguistic and grammatical patterns in their online writing, and I expect that a model trained over a larger corpus and with creative improvements could be quite effective at attributing authorship by gender, even in these short-form mediums.  What the application of such a model would be, and the ethical concerns around its usage, is another discussion worth having."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
