{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('final_dataset_cleaned_fourplus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought it might be useful to create a binary variable for whether a comment was highly upvoted.  This will allow us to use classification methods to predict comment success.  I thought 10 was an appropriate threshold in this case because this is a liminal number of upvotes between buried and salient in subreddits of this size.  It's about right as indicative of comment success. A 20 upvote comment is definitely well liked, while 5 is meh or may be controversial.  This target will also serve to normalize against the hugely disproportionate pull of a few comments that get massive upvotes on an analysis like this.  It reduces things to good comment / bad comment, which scale more accurately reflects the reality of the plus/minus quality of comments than the upvote score likely does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "elite_threshold = 10\n",
    "\n",
    "comments['elitecomment'] = comments['score'].map(lambda x: 1 if x>=elite_threshold else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>word length</th>\n",
       "      <th>elitecomment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was 23 but I went with my ~29 year old cowor...</td>\n",
       "      <td>1545243578</td>\n",
       "      <td>ec4lbi4</td>\n",
       "      <td>t3_a7oy9v</td>\n",
       "      <td>1</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Portland, OR. The city itself is now unafforda...</td>\n",
       "      <td>1545243546</td>\n",
       "      <td>ec4la0n</td>\n",
       "      <td>t3_a7mkui</td>\n",
       "      <td>1</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nope. \"the cats goodbye\" watch how a cat says ...</td>\n",
       "      <td>1545243536</td>\n",
       "      <td>ec4l9lm</td>\n",
       "      <td>t3_a7fe60</td>\n",
       "      <td>1</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Drunk as fuck me during an unintended one nigh...</td>\n",
       "      <td>1545243524</td>\n",
       "      <td>ec4l90i</td>\n",
       "      <td>t3_a79zu9</td>\n",
       "      <td>1</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There was this one time when I went over one o...</td>\n",
       "      <td>1545243449</td>\n",
       "      <td>ec4l5g6</td>\n",
       "      <td>t3_a7kmvc</td>\n",
       "      <td>1</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  created_utc       id  \\\n",
       "0  I was 23 but I went with my ~29 year old cowor...   1545243578  ec4lbi4   \n",
       "1  Portland, OR. The city itself is now unafforda...   1545243546  ec4la0n   \n",
       "2  nope. \"the cats goodbye\" watch how a cat says ...   1545243536  ec4l9lm   \n",
       "3  Drunk as fuck me during an unintended one nigh...   1545243524  ec4l90i   \n",
       "4  There was this one time when I went over one o...   1545243449  ec4l5g6   \n",
       "\n",
       "   parent_id  score subreddit  word length  elitecomment  \n",
       "0  t3_a7oy9v      1    AskMen           23             0  \n",
       "1  t3_a7mkui      1    AskMen           36             0  \n",
       "2  t3_a7fe60      1    AskMen           28             0  \n",
       "3  t3_a79zu9      1    AskMen           16             0  \n",
       "4  t3_a7kmvc      1    AskMen          192             0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit\n",
       "AskMen      0.148941\n",
       "AskWomen    0.157396\n",
       "Name: elitecomment, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.groupby('subreddit')['elitecomment'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 15% of comments meet the 10 upvote criterion, and it looks like these are pretty evenly distributed between the two subreddits.  Now to see what n-grams are predictive of upvotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams and Upvotes by Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "men = comments[comments['subreddit']=='AskMen'].copy()\n",
    "women = comments[comments['subreddit']=='AskWomen'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xm = men['body']\n",
    "ym = men['elitecomment']\n",
    "Xw = women['body']\n",
    "yw = women['elitecomment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xm_train, Xm_test, ym_train, ym_test = train_test_split(Xm,ym,shuffle=True,stratify=ym)\n",
    "Xw_train, Xw_test, yw_train, yw_test = train_test_split(Xw,yw,shuffle=True,stratify=yw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Gridsearch for AskMen words predictive of elite comment status\n",
    "\n",
    "pipemen = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('model', LogisticRegression())\n",
    "     ])\n",
    "\n",
    "params = {\n",
    "    'vect__ngram_range':[(1,3)],\n",
    "    'vect__min_df':[2,5],\n",
    "    'model__penalty':['l2','l1'],\n",
    "    'model__C':[0.1, 1, 10],\n",
    "}\n",
    "\n",
    "gs_men = GridSearchCV(pipemen, params, cv=3, verbose=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__ngram_range': [(1, 3)], 'vect__min_df': [2, 5], 'model__penalty': ['l2', 'l1'], 'model__C': [0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_men.fit(Xm_train, ym_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "          ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "          stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "          token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "          vocabulary=None)),\n",
       " ('model',\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_men.best_estimator_.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8510574277854195"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_men.score(Xm_train, ym_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.851063829787234"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_men.score(Xm_test, ym_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a coefficient matrix.  Note that the coefficients are significantly smaller than those seen in the Subreddit Classification LogReg (top predictors had coefficients of 8-10 there).  This makes sense when you consider that these coefficients correspond directly to increase in the likelihood of the result, and a comment containing a word like 'wife' should effect the likelihood of classification into AskMen a lot more powerfully than a comment containing a word like 'she' affecting whether a comment has more than 10 upvotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefsm = pd.DataFrame(gs_men.best_estimator_.steps[1][1].coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefsm.columns=gs_men.best_estimator_.steps[0][1].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>0.779843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>her</th>\n",
       "      <td>0.695038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>0.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me</th>\n",
       "      <td>0.418301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>0.394249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.387257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.351974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>0.317279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>who</th>\n",
       "      <td>0.302320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman</th>\n",
       "      <td>0.298675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>men</th>\n",
       "      <td>0.279073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>0.277495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>0.272559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actually</th>\n",
       "      <td>0.272541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>when</th>\n",
       "      <td>0.268166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>while</th>\n",
       "      <td>0.261607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>0.254331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.250836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst</th>\n",
       "      <td>0.248851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.241226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saying</th>\n",
       "      <td>0.236877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>someone</th>\n",
       "      <td>0.228349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>0.226954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>re</th>\n",
       "      <td>0.226853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>0.226011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>being</th>\n",
       "      <td>0.220972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>every</th>\n",
       "      <td>0.216533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>0.216501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has</th>\n",
       "      <td>0.215250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attractive</th>\n",
       "      <td>0.215045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>others</th>\n",
       "      <td>-0.128665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>then</th>\n",
       "      <td>-0.130254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>since</th>\n",
       "      <td>-0.130884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nice</th>\n",
       "      <td>-0.132782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>half</th>\n",
       "      <td>-0.133123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>-0.133753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>-0.134802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>more</th>\n",
       "      <td>-0.137391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phone</th>\n",
       "      <td>-0.140737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>though</th>\n",
       "      <td>-0.144694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wear</th>\n",
       "      <td>-0.144718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on my</th>\n",
       "      <td>-0.149732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>or</th>\n",
       "      <td>-0.150453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>few</th>\n",
       "      <td>-0.155479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>least</th>\n",
       "      <td>-0.155785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keep</th>\n",
       "      <td>-0.158859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>father</th>\n",
       "      <td>-0.159544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car</th>\n",
       "      <td>-0.167412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what you</th>\n",
       "      <td>-0.168312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>met</th>\n",
       "      <td>-0.176234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hair</th>\n",
       "      <td>-0.179463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maybe</th>\n",
       "      <td>-0.181377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>down</th>\n",
       "      <td>-0.182996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>take</th>\n",
       "      <td>-0.190694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amp</th>\n",
       "      <td>-0.199952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>-0.202898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if</th>\n",
       "      <td>-0.208593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>him</th>\n",
       "      <td>-0.218672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what</th>\n",
       "      <td>-0.239289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>much</th>\n",
       "      <td>-0.319288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191896 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "she         0.779843\n",
       "her         0.695038\n",
       "we          0.423000\n",
       "me          0.418301\n",
       "they        0.394249\n",
       "to          0.387257\n",
       "this        0.351974\n",
       "that        0.317279\n",
       "who         0.302320\n",
       "woman       0.298675\n",
       "men         0.279073\n",
       "sex         0.277495\n",
       "date        0.272559\n",
       "actually    0.272541\n",
       "when        0.268166\n",
       "while       0.261607\n",
       "man         0.254331\n",
       "the         0.250836\n",
       "worst       0.248851\n",
       "of          0.241226\n",
       "saying      0.236877\n",
       "someone     0.228349\n",
       "was         0.226954\n",
       "re          0.226853\n",
       "your        0.226011\n",
       "being       0.220972\n",
       "every       0.216533\n",
       "are         0.216501\n",
       "has         0.215250\n",
       "attractive  0.215045\n",
       "...              ...\n",
       "others     -0.128665\n",
       "then       -0.130254\n",
       "since      -0.130884\n",
       "nice       -0.132782\n",
       "half       -0.133123\n",
       "but        -0.133753\n",
       "go         -0.134802\n",
       "more       -0.137391\n",
       "phone      -0.140737\n",
       "though     -0.144694\n",
       "wear       -0.144718\n",
       "on my      -0.149732\n",
       "or         -0.150453\n",
       "few        -0.155479\n",
       "least      -0.155785\n",
       "keep       -0.158859\n",
       "father     -0.159544\n",
       "car        -0.167412\n",
       "what you   -0.168312\n",
       "met        -0.176234\n",
       "hair       -0.179463\n",
       "maybe      -0.181377\n",
       "down       -0.182996\n",
       "take       -0.190694\n",
       "amp        -0.199952\n",
       "get        -0.202898\n",
       "if         -0.208593\n",
       "him        -0.218672\n",
       "what       -0.239289\n",
       "much       -0.319288\n",
       "\n",
       "[191896 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefsm.T.sort_values(0,ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AskWomen Upvotes GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gridsearch for AskWomen words predictive of elite comment status\n",
    "pipewomen = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('model', LogisticRegression())\n",
    "     ])\n",
    "\n",
    "params = {\n",
    "    'vect__ngram_range':[(1,3)],\n",
    "    'vect__min_df':[2,5],\n",
    "    'model__penalty':['l2','l1'],\n",
    "    'model__C':[0.1, 1, 10],\n",
    "}\n",
    "\n",
    "gs_women = GridSearchCV(pipewomen, params, cv=3, verbose=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:  3.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__ngram_range': [(1, 3)], 'vect__min_df': [2, 5], 'model__penalty': ['l2', 'l1'], 'model__C': [0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_women.fit(Xw_train, yw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "          ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "          stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "          token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "          vocabulary=None)),\n",
       " ('model',\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_women.best_estimator_.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8426112803199882"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_women.score(Xw_train, yw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8425803610744166"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_women.score(Xw_test, yw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>1.055887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.927168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>0.883008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>women</th>\n",
       "      <td>0.881722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>her</th>\n",
       "      <td>0.862972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>0.818250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0.705215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>men</th>\n",
       "      <td>0.694496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.686267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.630533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>0.618116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>0.562062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>who</th>\n",
       "      <td>0.552435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>his</th>\n",
       "      <td>0.469677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman</th>\n",
       "      <td>0.462457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.456040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>0.431770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>how</th>\n",
       "      <td>0.431155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>about</th>\n",
       "      <td>0.401216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>him</th>\n",
       "      <td>0.394799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>0.392314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.387664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>0.383716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guy</th>\n",
       "      <td>0.383304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>their</th>\n",
       "      <td>0.375594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.373282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.358613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>0.347212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>re</th>\n",
       "      <td>0.343262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.342095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not sure</th>\n",
       "      <td>-0.167913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>used to</th>\n",
       "      <td>-0.170981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure</th>\n",
       "      <td>-0.174347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>-0.180844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>christmas</th>\n",
       "      <td>-0.181346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like to</th>\n",
       "      <td>-0.181426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>days</th>\n",
       "      <td>-0.181577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>months</th>\n",
       "      <td>-0.186813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hair</th>\n",
       "      <td>-0.190343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ve</th>\n",
       "      <td>-0.192211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am</th>\n",
       "      <td>-0.194498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depends on</th>\n",
       "      <td>-0.196792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boyfriend</th>\n",
       "      <td>-0.199655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would be</th>\n",
       "      <td>-0.200952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friends</th>\n",
       "      <td>-0.201343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>music</th>\n",
       "      <td>-0.201627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>few</th>\n",
       "      <td>-0.201655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sometimes</th>\n",
       "      <td>-0.202412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>-0.203140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in my</th>\n",
       "      <td>-0.215462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>-0.221526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>-0.222419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>really</th>\n",
       "      <td>-0.232504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>-0.236511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>though</th>\n",
       "      <td>-0.247506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>week</th>\n",
       "      <td>-0.251468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depends</th>\n",
       "      <td>-0.252134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usually</th>\n",
       "      <td>-0.292006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>-0.294473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>-0.517453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252113 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "she         1.055887\n",
       "he          0.927168\n",
       "was         0.883008\n",
       "women       0.881722\n",
       "her         0.862972\n",
       "that        0.818250\n",
       "you         0.705215\n",
       "men         0.694496\n",
       "the         0.686267\n",
       "to          0.630533\n",
       "they        0.618116\n",
       "people      0.562062\n",
       "who         0.552435\n",
       "his         0.469677\n",
       "woman       0.462457\n",
       "this        0.456040\n",
       "are         0.431770\n",
       "how         0.431155\n",
       "about       0.401216\n",
       "him         0.394799\n",
       "man         0.392314\n",
       "all         0.387664\n",
       "said        0.383716\n",
       "guy         0.383304\n",
       "their       0.375594\n",
       "like        0.373282\n",
       "is          0.358613\n",
       "your        0.347212\n",
       "re          0.343262\n",
       "of          0.342095\n",
       "...              ...\n",
       "not sure   -0.167913\n",
       "used to    -0.170981\n",
       "sure       -0.174347\n",
       "it         -0.180844\n",
       "christmas  -0.181346\n",
       "like to    -0.181426\n",
       "days       -0.181577\n",
       "months     -0.186813\n",
       "hair       -0.190343\n",
       "ve         -0.192211\n",
       "am         -0.194498\n",
       "depends on -0.196792\n",
       "boyfriend  -0.199655\n",
       "would be   -0.200952\n",
       "friends    -0.201343\n",
       "music      -0.201627\n",
       "few        -0.201655\n",
       "sometimes  -0.202412\n",
       "work       -0.203140\n",
       "in my      -0.215462\n",
       "love       -0.221526\n",
       "would      -0.222419\n",
       "really     -0.232504\n",
       "year       -0.236511\n",
       "though     -0.247506\n",
       "week       -0.251468\n",
       "depends    -0.252134\n",
       "usually    -0.292006\n",
       "have       -0.294473\n",
       "my         -0.517453\n",
       "\n",
       "[252113 rows x 1 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefsw = pd.DataFrame(gs_women.best_estimator_.steps[1][1].coef_)\n",
    "coefsw.columns=gs_women.best_estimator_.steps[0][1].get_feature_names()\n",
    "coefsw.T.sort_values(0,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefsm.T.sort_values(0,ascending=False).to_csv('coefsm_upvotes.csv')\n",
    "coefsw.T.sort_values(0,ascending=False).to_csv('coefsw_upvotes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I tabulated the top associated coefficients for predicting upvoted comments in each subreddit and then calculated the difference to get a sense of which coefficients were particularly different in their direction of influence on upvotes in one sub versus the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefsw.rename(columns={'Unnamed: 0':'gram','0':'wcoef'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefsm.rename(columns={'Unnamed: 0':'gram','0':'mcoef'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefsmerged = coefsw.merge(coefsm,how='inner',on='gram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefsmerged['diff'] = coefsmerged['wcoef'] - coefsmerged['mcoef']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefsmerged.sort_values('diff').to_csv('coefsmerged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gram</th>\n",
       "      <th>wcoef</th>\n",
       "      <th>mcoef</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107067</th>\n",
       "      <td>my</td>\n",
       "      <td>-0.517453</td>\n",
       "      <td>-0.043388</td>\n",
       "      <td>-0.474065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93529</th>\n",
       "      <td>we</td>\n",
       "      <td>-0.010835</td>\n",
       "      <td>0.423000</td>\n",
       "      <td>-0.433836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107060</th>\n",
       "      <td>really</td>\n",
       "      <td>-0.232504</td>\n",
       "      <td>0.145319</td>\n",
       "      <td>-0.377823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107061</th>\n",
       "      <td>year</td>\n",
       "      <td>-0.236511</td>\n",
       "      <td>0.084623</td>\n",
       "      <td>-0.321133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107056</th>\n",
       "      <td>work</td>\n",
       "      <td>-0.203140</td>\n",
       "      <td>0.112019</td>\n",
       "      <td>-0.315160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106997</th>\n",
       "      <td>never</td>\n",
       "      <td>-0.121444</td>\n",
       "      <td>0.183579</td>\n",
       "      <td>-0.305023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97833</th>\n",
       "      <td>actually</td>\n",
       "      <td>-0.013538</td>\n",
       "      <td>0.272541</td>\n",
       "      <td>-0.286079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107057</th>\n",
       "      <td>in my</td>\n",
       "      <td>-0.215462</td>\n",
       "      <td>0.066717</td>\n",
       "      <td>-0.282180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106908</th>\n",
       "      <td>best</td>\n",
       "      <td>-0.085222</td>\n",
       "      <td>0.196334</td>\n",
       "      <td>-0.281556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106996</th>\n",
       "      <td>partner</td>\n",
       "      <td>-0.120950</td>\n",
       "      <td>0.157021</td>\n",
       "      <td>-0.277971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            gram     wcoef     mcoef      diff\n",
       "107067        my -0.517453 -0.043388 -0.474065\n",
       "93529         we -0.010835  0.423000 -0.433836\n",
       "107060    really -0.232504  0.145319 -0.377823\n",
       "107061      year -0.236511  0.084623 -0.321133\n",
       "107056      work -0.203140  0.112019 -0.315160\n",
       "106997     never -0.121444  0.183579 -0.305023\n",
       "97833   actually -0.013538  0.272541 -0.286079\n",
       "107057     in my -0.215462  0.066717 -0.282180\n",
       "106908      best -0.085222  0.196334 -0.281556\n",
       "106996   partner -0.120950  0.157021 -0.277971"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefsmerged.sort_values('diff').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gram</th>\n",
       "      <th>wcoef</th>\n",
       "      <th>mcoef</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he</td>\n",
       "      <td>0.927168</td>\n",
       "      <td>-0.020592</td>\n",
       "      <td>0.947760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>women</td>\n",
       "      <td>0.881722</td>\n",
       "      <td>0.143953</td>\n",
       "      <td>0.737769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>was</td>\n",
       "      <td>0.883008</td>\n",
       "      <td>0.226954</td>\n",
       "      <td>0.656054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>him</td>\n",
       "      <td>0.394799</td>\n",
       "      <td>-0.218672</td>\n",
       "      <td>0.613471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>you</td>\n",
       "      <td>0.705215</td>\n",
       "      <td>0.104216</td>\n",
       "      <td>0.600999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>that</td>\n",
       "      <td>0.818250</td>\n",
       "      <td>0.317279</td>\n",
       "      <td>0.500971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the</td>\n",
       "      <td>0.686267</td>\n",
       "      <td>0.250836</td>\n",
       "      <td>0.435431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>down</td>\n",
       "      <td>0.251346</td>\n",
       "      <td>-0.182996</td>\n",
       "      <td>0.434342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>men</td>\n",
       "      <td>0.694496</td>\n",
       "      <td>0.279073</td>\n",
       "      <td>0.415424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>how</td>\n",
       "      <td>0.431155</td>\n",
       "      <td>0.038642</td>\n",
       "      <td>0.392513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     gram     wcoef     mcoef      diff\n",
       "1      he  0.927168 -0.020592  0.947760\n",
       "3   women  0.881722  0.143953  0.737769\n",
       "2     was  0.883008  0.226954  0.656054\n",
       "19    him  0.394799 -0.218672  0.613471\n",
       "6     you  0.705215  0.104216  0.600999\n",
       "5    that  0.818250  0.317279  0.500971\n",
       "8     the  0.686267  0.250836  0.435431\n",
       "44   down  0.251346 -0.182996  0.434342\n",
       "7     men  0.694496  0.279073  0.415424\n",
       "17    how  0.431155  0.038642  0.392513"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefsmerged.sort_values('diff',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gram</th>\n",
       "      <th>mcoef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>191886</th>\n",
       "      <td>hair</td>\n",
       "      <td>-0.179463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191887</th>\n",
       "      <td>maybe</td>\n",
       "      <td>-0.181377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191888</th>\n",
       "      <td>down</td>\n",
       "      <td>-0.182996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191889</th>\n",
       "      <td>take</td>\n",
       "      <td>-0.190694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191890</th>\n",
       "      <td>amp</td>\n",
       "      <td>-0.199952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191891</th>\n",
       "      <td>get</td>\n",
       "      <td>-0.202898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191892</th>\n",
       "      <td>if</td>\n",
       "      <td>-0.208593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191893</th>\n",
       "      <td>him</td>\n",
       "      <td>-0.218672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191894</th>\n",
       "      <td>what</td>\n",
       "      <td>-0.239289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191895</th>\n",
       "      <td>much</td>\n",
       "      <td>-0.319288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         gram     mcoef\n",
       "191886   hair -0.179463\n",
       "191887  maybe -0.181377\n",
       "191888   down -0.182996\n",
       "191889   take -0.190694\n",
       "191890    amp -0.199952\n",
       "191891    get -0.202898\n",
       "191892     if -0.208593\n",
       "191893    him -0.218672\n",
       "191894   what -0.239289\n",
       "191895   much -0.319288"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefsm.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(coefsmerged.sort_values(mcoef))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are remarkable numbers.  The train and test score are almost a perfect match, and 0.85 is substantially stronger than the numbers seen in my subreddit classification models.  After thinking about what might be allowing this predictive power on what seems like a harder classification problem on the face of it, I realized (post presentation!) that the baseline accuracy could be too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    57015\n",
       "1    10339\n",
       "Name: elitecomment, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments['elitecomment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8464976096445646"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "57015/(10339+57015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit  elitecomment\n",
       "AskMen     0               26399\n",
       "           1                4620\n",
       "AskWomen   0               30616\n",
       "           1                5719\n",
       "Name: elitecomment, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.groupby('subreddit')['elitecomment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8510590283374706"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "26399/(26399+4620)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.842603550295858"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30616/(5719+30616)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alas.  The numbers correspond almost exactly to the observed accuracy values from the GridSearch.  Shouldve guessed from the stopwords topping the lists. Great example of how it's easy to run with the wrong data narrative if you're not careful.  If I have some more time, I'm going to re-run the gridsearch with a higher threshold to generate more equal classes and hopefully get more useful numbers.  Undersampling or Bootstrapping while maintaining the 10+ threshold would probably be ideal, but for the sake of time, I'm going to lower the threshold a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "elite_threshold = 3\n",
    "\n",
    "comments['elitecomment'] = comments['score'].map(lambda x: 1 if x>=elite_threshold else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit  elitecomment\n",
       "AskMen     0               19603\n",
       "           1               11416\n",
       "AskWomen   0               20898\n",
       "           1               15437\n",
       "Name: elitecomment, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.groupby('subreddit')['elitecomment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipemen3 = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('model', LogisticRegression())\n",
    "     ])\n",
    "\n",
    "params = {\n",
    "    'vect__ngram_range':[(1,3)],\n",
    "    'vect__min_df':[2,5],\n",
    "    'model__penalty':['l2','l1'],\n",
    "    'model__C':[0.1, 1, 10],\n",
    "}\n",
    "\n",
    "gs_men3 = GridSearchCV(pipemen3, params, cv=3, verbose=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__ngram_range': [(1, 3)], 'vect__min_df': [2, 5], 'model__penalty': ['l2', 'l1'], 'model__C': [0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_men3.fit(Xm_train, ym_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6547025447042641"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_men3.score(Xm_train, ym_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6288845905867182"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_men3.score(Xm_test, ym_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6321713051049695"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "19603/(19603+11406)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a lot of evidence that this leads to anything meaningful.  I think the correct approach might be to use balanced classes and leave a gap, so maybe 2 and fewer upvotes against 100 or more upvotes.  Perhaps there'd be more traction there.  Worth trying at a later point.  Getting on plane now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
